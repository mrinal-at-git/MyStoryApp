<?xml version="1.0" encoding="utf-8"?>
<resources>

    <string-array name="stories_content">

        <item>
            The selection sort algorithm sorts an array by repeatedly finding the minimum element
            (considering ascending order) from unsorted part and putting it at the beginning.
            The algorithm maintains two sub arrays in a given array.

            1) The sub array which is already sorted.
            2) Remaining sub array which is unsorted.

            In every iteration of selection sort, the minimum element (considering ascending order)
            from the unsorted sub array is picked and moved to the sorted sub array.


            Time Complexity: O(n2) as there are two nested loops.
            Auxiliary Space: O(1)
            The good thing about selection sort is it never makes more than O(n) swaps
            and can be useful when memory write is a costly operation.</item>

        <item>

            Bubble Sort is the simplest sorting algorithm that works by repeatedly swapping the adjacent elements if they are in wrong order.


            Worst and Average Case Time Complexity: O(n*n). Worst case occurs when array is reverse sorted.
            Best Case Time Complexity: O(n). Best case occurs when array is already sorted.
            Auxiliary Space: O(1)
            Boundary Cases: Bubble sort takes minimum time (Order of n) when elements are already sorted.
            Sorting In Place: Yes
            Stable:Yes</item>


        <item>Insertion sort is a simple sorting algorithm that works the way we sort playing cards in our hands.

              Algorithm
            // Sort an arr[] of size n
            insertionSort(arr, n)
            Loop from i = 1 to n-1.
            ……a) Pick element arr[i] and insert it into sorted sequence arr[0…i-1]

            Time Complexity: O(n*2)
            Auxiliary Space: O(1)
            Boundary Cases: Insertion sort takes maximum time to sort if elements are sorted in reverse order.
            And it takes minimum time (Order of n) when elements are already sorted.
            Algorithmic Paradigm: Incremental Approach
            Sorting In Place: Yes
            Stable: Yes
            Online: Yes

            Uses: Insertion sort is used when number of elements is small.
            It can also be useful when input array is almost sorted, only few elements are misplaced in complete big array.</item>


        <item>
            Merge Sort is a Divide and Conquer algorithm. It divides input array in two halves, calls itself for the two halves and then merges the two sorted halves. The merge() function is used for merging two halves. The merge(arr, l, m, r) is key process that assumes that arr[l..m] and arr[m+1..r] are sorted and merges the two sorted sub-arrays into one. See following C implementation for details.

           MergeSort(arr[], l,  r)
           If r > l
                1. Find the middle point to divide the array into two halves:
                       middle m = (l+r)/2
                2. Call mergeSort for first half:
                       Call mergeSort(arr, l, m)
               3. Call mergeSort for second half:
                       Call mergeSort(arr, m+1, r)
               4. Merge the two halves sorted in step 2 and 3:
                       Call merge(arr, l, m, r)


            Time Complexity: Sorting arrays on different machines.
            Merge Sort is a recursive algorithm and time complexity can be expressed as following recurrence relation.

            T(n) = 2T(n/2) + \Theta(n)

            The above recurrence can be solved either using Recurrence Tree method or Master method.
            It falls in case II of Master Method and solution of the recurrence is \Theta(nLogn).
            Time complexity of Merge Sort is \Theta(nLogn) in all 3 cases (worst, average and best) as merge sort always divides the array into two halves and take linear time to merge two halves.

            Auxiliary Space: O(n)
            Algorithmic Paradigm: Divide and Conquer
            Sorting In Place: No in a typical implementation
            Stable: Yes </item>


        <item>
            Like Merge Sort, QuickSort is a Divide and Conquer algorithm. It picks an element as pivot and partitions the given array around the picked pivot. There are many different versions of quickSort that pick pivot in different ways.

            Always pick first element as pivot.
            Always pick last element as pivot (implemented below)
            Pick a random element as pivot.
            Pick median as pivot.

            The key process in quickSort is partition().
            Target of partitions is, given an array and an element x of array as pivot,
            put x at its correct position in sorted array and put all smaller elements (smaller than x) before x,
            and put all greater elements (greater than x) after x. All this should be done in linear time.


quickSort(arr[], low, high)
{
    if (low less_than high)
    {
        /* pi is partitioning index, arr[pi] is now
           at right place */
        pi = partition(arr, low, high);

        quickSort(arr, low, pi - 1);  // Before pi
        quickSort(arr, pi + 1, high); // After pi
    }
}</item>


        <item>
            The lower bound for Comparison based sorting algorithm (Merge Sort, Heap Sort, Quick-Sort .. etc) is Ω(nLogn), i.e., they cannot do better than nLogn.
            Counting sort is a linear time sorting algorithm that sort in O(n+k) time when elements are in range from 1 to k.
            What if the elements are in range from 1 to n2?
            We can’t use counting sort because counting sort will take O(n2) which is worse than comparison based sorting algorithms. Can we sort such an array in linear time?
            Radix Sort is the answer. The idea of Radix Sort is to do digit by digit sort starting from least significant digit to most significant digit. Radix sort uses counting sort as a subroutine to sort.


            The Radix Sort Algorithm
            1) Do following for each digit i where i varies from least significant digit to the most significant digit.
            ……….a) Sort input array using counting sort (or any stable sort) according to the i’th digit.
        </item>

        <item>

            Cycle sort is an in-place sorting Algorithm, unstable sorting algorithm,
            a comparison sort that is theoretically optimal in terms of the total number of writes to the original array.

            It is optimal in terms of number of memory writes. It minimizes the number of memory writes to sort
            (Each value is either written zero times, if it’s already in its correct position, or written one time to its correct position.)
            It is based on the idea that array to be sorted can be divided into cycles. Cycles can be visualized as a graph.
            We have n nodes and an edge directed from node i to node j if the element at i-th index must be present at j-th index in the sorted array.Cycle in arr[] = {2, 4, 5, 1, 3}.


            We one by one consider all cycles. We first consider the cycle that includes first element.
            We find correct position of first element, place it at its correct position, say j.
            We consider old value of arr[j] and find its correct position,
            we keep doing this till all elements of current cycle are placed at correct position, i.e.,
            we don’t come back to cycle starting point.</item>

        <item>
            Counting sort is a sorting technique based on keys between a specific range.
            It works by counting the number of objects having distinct key values (kind of hashing).
            Then doing some arithmetic to calculate the position of each object in the output sequence.


                   Points to be noted:
            1. Counting sort is efficient if the range of input data is not significantly greater than the number of objects to be sorted. Consider the situation where the input sequence is between range 1 to 10K and the data is 10, 5, 10K, 5K.
            2. It is not a comparison based sorting. It running time complexity is O(n) with space proportional to the range of data.
            3. It is often used as a sub-routine to another sorting algorithm like radix sort.
            4. Counting sort uses a partial hashing to count the occurrence of the data object in O(1).
            5. Counting sort can be extended to work for negative inputs also.</item>

        <item>
            Heap sort is a comparison based sorting technique based on Binary Heap data structure. It is similar to selection sort where we first find the maximum element and place the maximum element at the end. We repeat the same process for remaining element.

            What is Binary Heap?
            Let us first define a Complete Binary Tree. A complete binary tree is a binary tree in which every level, except possibly the last, is completely filled, and all nodes are as far left as possible (Source Wikipedia)
            A Binary Heap is a Complete Binary Tree where items are stored in a special order such that value in a parent node is greater(or smaller) than the values in its two children nodes. The former is called as max heap and the latter is called min heap. The heap can be represented by binary tree or array.


            Why array based representation for Binary Heap?
            Since a Binary Heap is a Complete Binary Tree, it can be easily represented as array and array based representation is space efficient. If the parent node is stored at index I, the left child can be calculated by 2 * I + 1 and right child by 2 * I + 2 (assuming the indexing starts at 0).

            Heap Sort Algorithm for sorting in increasing order:
            1. Build a max heap from the input data.
            2. At this point, the largest item is stored at the root of the heap. Replace it with the last item of the heap followed by reducing the size of heap by 1. Finally, heapify the root of tree.
            3. Repeat above steps while size of heap is greater than 1.</item>

</string-array>



</resources>